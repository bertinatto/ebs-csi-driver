= Radkube
Shane Siebken <shane.siebken@RadiantSolutions.com>
:source-highlighter: rouge

Documentation for the shared development sandbox of Radiant
Charlottesville, powered by Kubernetes.

== Environment Creation
The Kubernetes infrastructure is structured using the modules
and toolsets exposed by the https://typhoon.psdn.io[Typhoon] project.

Instructions followed for the initial AWS cluster can be found 
https://typhoon.psdn.io/cl/aws/[here]. The configuration for this
environment, with the exception of TLS configuration and other
secrets, are housed in this repository, primarily in the
link:./cluster.tf[Cluster configuration file].

== Further Configuration
The first step for making the cluster useful and useable is enabling
developer access and usage. https://github.com/dexidp/dex[Dex IDP] is
used to provide a simple Single Sign On experience for developers, as
well as cluster admins.

=== Dex IDP Cluster Auth
Generalized instructions for using Dex IDP as an OIDC identity provider
for Kubernetes can be found 
https://github.com/dexidp/dex/blob/master/Documentation/kubernetes.md[here].
Managing this configuration for Radkube is simultaneously more and less complex
than in a typical cluster. The major factor in this complexity is the usage of
bootkube.

.Bootkube
****
Bootkube is a simple cluster bootstrapping tool which can be used to establish a
self-hosted Kubernetes cluster. This means that all of the components (with the
exception of node Kubelets) are managed as Kubernetes objects, by the cluster
itself.
****

For instance, the Kubernetes API server that needs command changes to support
OIDC authentication / authorization, is managed as a Daemonset, and can have
its command specification edited in the Daemonset configuration. Addtionally,
the necessary TLS assets which are needed by the API server to trust Dex IDP
(and needed by Dex IDP for secure TLS configuration) can be managed as Kubernetes
secrets, and thus do not need to be distributed onto host machines for use
by the API server.

.API Server Configuration
[#apiserver]
====
Deploying Dex IDP as an auth provider requires the addition of several flags
to the Kubernetes API server, as well as additional TLS assets (the CA which
serves to identify Dex IDP must be mounted to the API server. TODO: This dependency
could be avoided in the future by issuing certificates for Dex off of the
CA used for token issuance within the cluster.)

The process to add this configuration is as follows (assuming that kubectl is
installed and configured per Typhhon creation instructions):

1. Generate TLS assets per dex-idp instructions
https://github.com/dexidp/dex/blob/master/Documentation/kubernetes.md#generate-tls-assets[here].
2. Create a configmap with the new Dex ca.pem:
+
[source, bash]
----
kubectl create --namespace kube-system configmap dex-ca  --from-file=/home/user/go/src/github.com/dexidp/dex/examples/k8s/ssl/ca.pem`
----
+
3. Patch the existing apiserver deployment with the updated command:
+
[source, bash]
----
cd /path/to/radkube/oidc
./makepatch.sh
kubectl patch ds kube-apiserver --namespace kube-system \
  --patch "$(cat ./patch.json)"
----
+
[NOTE]
======
Be sure to review the generated patch file. Patching system components can cause
major cluster downtime.
======
+
4. Wait for API server Daemonset to fully roll out new command.
====

.Dex IDP Configuration
[#dexidp]
====
Next up is deploying the Dex IDP OIDC provider, configured against apropriate
provider backend auth providers (currently the GitHub connector).

This process should be cleaned up a bit moving forward, to use a public trusted
root CA like Let's Encrypt via some mechanism like 
https://github.com/jetstack/kube-lego[kube-lego]. TODO: Confirm this can actually
work as expected.

1. Create cluster secrets:
https://github.com/dexidp/dex/blob/master/Documentation/kubernetes.md#create-cluster-secrets
2. Deploy the Dex server:
https://github.com/dexidp/dex/blob/master/Documentation/kubernetes.md#deploy-the-dex-server
. NOTE: In the current Radkube configuration, an ingress resource should be added and configured
for the Dex server. This ingress resource should pass through TLS to Dex, as it terminates
SSL with the configured assets. See the section on NGINX Ingress for further instructions.

As long as all cluster level assets (configmaps, secrets, etc.) have been properly configured
and referenced, all system components should be chugging along happily, although token
auth. will not work until an Ingress resource is created for the Dex server.
====

.NGINX Configuration
[#nginx]
====
This component is installed with Helm. The values file can be found in the
`ingress` directory. Installation is as follows:
[source, bash]
----
helm install stable/nginx-ingress --name nginx-ingress --namespace nginx-ingress --values ./ingress/values.yaml
----
[NOTE]
======
Ensure that an appropriate namespace exists before attempting this installation, or create
one.
======
The NGINX Ingress is installed as a Daemonset to allow low-maintainance (but flexible)
deployment on all workers, allowing for externally managed load-balancers to
effectively route traffic to worker pools. This configuration is particularly geared
towards future plans of cross-account cluster expansion.
====

.Cert Manager
[#certManager]
====
https://github.com/jetstack/cert-manager[Cert Manager] is a project which automatically creates and manages
various TLS certificates as native Kubernetes objects. Cert Manager leans on Let's Encrypt,
a widely trusted, free Certificate Authority. Cert Manager can be configured to automatically
issue certificates for ingress resources, which will then be used to secure TLS traffic for that
resource. This configuration will be used in Radkube to protect any resources which do not need
unique TLS configurations (like gov't projects needing SSL passthrough to test 2-way SSL, and others).

1. Install Cert Manager with the recommended Helm chart:
+
[source, bash]
----
helm install \
    --name cert-manager \
    --namespace kube-system \
    stable/cert-manager
----
+
2. Create an issuer to get credentials issued:
. Follow the instructions
https://cert-manager.readthedocs.io/en/latest/reference/issuers/acme/dns01.html#supported-dns01-providers[here]
to configure a user with correct policies to create and edit Route53 DNS records.
. Create a Kubernetes secret in the kube-system namespace with the user's secret access key.:
[source, bash]
----
kubectl create secret generic \
--from-literal=secret-access-key=<secret_access_key> \
<secret-name>
----
riot-route53-credentials-secret
====

.Helm & Tiller
[#helmTiller]

TODO: Work out specifics of secure installation and record here